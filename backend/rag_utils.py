"""
Production-Ready RAG System (Enhanced)
======================================

A high-performance, scalable RAG (Retrieval-Augmented Generation) system designed for:
- 500GB+ document ingestion
- Sub-second retrieval latency
- Precise document-level citations
- Session-based conversation context (cleared on close/clear)
- Adaptive chunking based on document structure
- Query expansion and re-ranking for better relevance
- Multi-level retrieval strategy

PUBLIC API (DO NOT CHANGE SIGNATURES):
--------------------------------------
1. process_document(file_path: str, file_type: str = None) -> dict
2. get_answer_with_fallback(query: str, conversation_history: list = None, stream: bool = False)

ARCHITECTURE:
-------------
- Adaptive document processing with structure analysis
- Multi-strategy vector search with re-ranking
- Query expansion for better concept coverage
- Smart caching for frequently accessed documents
- Citation tracking with source metadata
- Metadata-based filtering for precision
"""

import os
import time
import hashlib
import re
from typing import List, Dict, Optional, Generator, Tuple, Any
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
import threading

from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader
from langchain_community.llms import Ollama
from langchain_community.embeddings import OllamaEmbeddings
from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from dotenv import load_dotenv
import warnings
warnings.filterwarnings('ignore', category=DeprecationWarning)

load_dotenv()

# ============================================================================
# CONFIGURATION
# ============================================================================

class Config:
    """Centralized configuration - modify these for tuning"""
    
    # Paths
    PERSIST_DIRECTORY = "./chroma_db"
    UPLOAD_DIR = "./uploads"
    
    # Ollama Settings
    OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
    OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "llama3.2")
    EMBEDDING_MODEL = "nomic-embed-text"
    
    # Adaptive Chunking Settings
    # Different chunk sizes for different content types
    CHUNK_SIZES = {
        'default': 1500,      # General documents
        'technical': 1200,    # Technical docs (tighter chunks)
        'narrative': 2000,    # Story/narrative content (larger chunks)
        'structured': 1000,   # Lists/structured content (smaller chunks)
    }
    CHUNK_OVERLAP = 250  # Increased for better context continuity
    
    # Multi-level Search Settings
    SEARCH_K_INITIAL = 15  # Initial retrieval (broader)
    SEARCH_K_RERANK = 5    # After re-ranking (top results)
    SIMILARITY_THRESHOLD = 0.3  # Normalized threshold (0-1)
    
    # Query Expansion Settings
    ENABLE_QUERY_EXPANSION = True
    EXPANSION_KEYWORDS = 3  # Number of expansion terms to generate
    
    # LLM Settings
    TEMPERATURE = 0.1  # Low for factual responses
    MAX_TOKENS = 1024  # Increased for comprehensive answers
    CONTEXT_WINDOW = 4096
    
    # Performance Settings
    BATCH_SIZE = 100  # Chunks per batch for embedding
    MAX_WORKERS = 4  # Parallel processing threads
    ENABLE_CACHING = True  # Cache frequent queries


# ============================================================================
# DATA CLASSES
# ============================================================================

@dataclass
class Citation:
    """Represents a source citation"""
    document_name: str
    page_number: Optional[int]
    chunk_index: int
    relevance_score: float
    content_preview: str


@dataclass
class SearchResult:
    """Represents a search result with metadata"""
    content: str
    citation: Citation
    metadata: Dict[str, Any]


# ============================================================================
# QUERY EXPANDER
# ============================================================================

class QueryExpander:
    """Expands queries to improve retrieval coverage"""
    
    def __init__(self):
        self.llm = Ollama(
            base_url=Config.OLLAMA_BASE_URL,
            model=Config.OLLAMA_MODEL,
            temperature=0.7,
            num_predict=100,
        )
    
    def expand_query(self, query: str) -> List[str]:
        """Generate related search terms for the query"""
        if not Config.ENABLE_QUERY_EXPANSION:
            return [query]
        
        try:
            prompt = f"""Generate {Config.EXPANSION_KEYWORDS} alternative search queries that would find the same information as the original query.
These should be DIRECT VARIATIONS of the original query, not completely different topics.

Rules:
- Keep the core meaning of the original query
- Use synonyms and related terms
- Rephrase the question in different ways
- Do NOT generate unrelated queries

Original query: {query}

Return only the queries, one per line, without numbering or explanation.

Alternative queries:"""
            
            response = self.llm.invoke(prompt)
            expanded = [query] + [q.strip() for q in response.split('\n') if q.strip()]
            return expanded[:Config.EXPANSION_KEYWORDS + 1]
        except Exception as e:
            print(f"‚ö†Ô∏è  Query expansion failed: {e}", flush=True)
            return [query]


# ============================================================================
# DOCUMENT PROCESSOR
# ============================================================================

class DocumentProcessor:
    """Handles document loading, chunking, and indexing with adaptive strategies"""
    
    def __init__(self):
        self.embeddings = OllamaEmbeddings(
            base_url=Config.OLLAMA_BASE_URL,
            model=Config.EMBEDDING_MODEL
        )
    
    def detect_content_type(self, text: str) -> str:
        """Detect document content type to choose optimal chunking strategy"""
        # Count patterns to determine content type
        list_pattern = len(re.findall(r'^\s*[\d\-\*‚Ä¢]\s+', text, re.MULTILINE))
        heading_pattern = len(re.findall(r'^#+\s+', text, re.MULTILINE))
        code_pattern = len(re.findall(r'```|def |class |function ', text, re.IGNORECASE))
        
        total_lines = len(text.split('\n'))
        
        # Determine type based on patterns
        if code_pattern > total_lines * 0.1:
            return 'technical'
        elif (list_pattern + heading_pattern) > total_lines * 0.2:
            return 'structured'
        elif heading_pattern > total_lines * 0.15:
            return 'structured'
        else:
            return 'narrative'
    
    def get_adaptive_splitter(self, content_type: str) -> RecursiveCharacterTextSplitter:
        """Get text splitter configured for content type"""
        chunk_size = Config.CHUNK_SIZES.get(content_type, Config.CHUNK_SIZES['default'])
        
        return RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=Config.CHUNK_OVERLAP,
            length_function=len,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
    
    def load_document(self, file_path: str, file_type: str) -> List[Any]:
        """Load document based on file type"""
        loaders = {
            'pdf': PyPDFLoader,
            'docx': Docx2txtLoader,
            'doc': Docx2txtLoader,
            'txt': TextLoader
        }
        
        loader_class = loaders.get(file_type.lower())
        if not loader_class:
            raise ValueError(f"Unsupported file type: {file_type}")
        
        loader = loader_class(file_path)
        return loader.load()
    
    def chunk_documents(self, documents: List[Any], file_path: str, file_type: str) -> List[Any]:
        """Split documents into chunks with adaptive strategy based on content"""
        # Combine all document text to analyze
        full_text = "\n\n".join([doc.page_content for doc in documents])
        
        # Detect content type and get appropriate splitter
        content_type = self.detect_content_type(full_text)
        print(f"üìä Detected content type: {content_type}", flush=True)
        
        splitter = self.get_adaptive_splitter(content_type)
        chunks = splitter.split_documents(documents)
        
        # Extract original filename from path
        original_name = os.path.basename(file_path)
        if '_' in original_name:
            # Remove UUID prefix if present
            parts = original_name.split('_', 1)
            if len(parts[0]) == 36:  # UUID length
                original_name = parts[1]
        
        # Add rich metadata to each chunk
        for i, chunk in enumerate(chunks):
            chunk.metadata.update({
                "chunk_index": i,
                "total_chunks": len(chunks),
                "source_file": file_path,
                "original_filename": original_name,
                "file_type": file_type,
                "content_type": content_type,
                "doc_id": hashlib.md5(file_path.encode()).hexdigest()[:8]
            })
        
        return chunks
    
    def index_chunks(self, chunks: List[Any]) -> int:
        """Index chunks into vector database with parallel batching"""
        if not chunks:
            return 0
        
        # Get or create vector database
        if os.path.exists(Config.PERSIST_DIRECTORY):
            vectordb = Chroma(
                persist_directory=Config.PERSIST_DIRECTORY,
                embedding_function=self.embeddings
            )
        else:
            vectordb = Chroma(
                persist_directory=Config.PERSIST_DIRECTORY,
                embedding_function=self.embeddings
            )
        
        # Parallel batch indexing for faster processing
        total = len(chunks)
        
        def index_batch(batch_data):
            """Index a batch of chunks"""
            batch_idx, batch = batch_data
            try:
                vectordb.add_documents(batch)
                progress = min((batch_idx + 1) * Config.BATCH_SIZE, total)
                print(f"üìä Indexed {progress}/{total} chunks ({int(progress/total*100)}%)", flush=True)
                return True
            except Exception as e:
                print(f"‚ùå Error indexing batch {batch_idx}: {e}", flush=True)
                return False
        
        # Create batches
        batches = []
        for i in range(0, total, Config.BATCH_SIZE):
            batch = chunks[i:i + Config.BATCH_SIZE]
            batch_idx = i // Config.BATCH_SIZE
            batches.append((batch_idx, batch))
        
        # Process batches in parallel
        with ThreadPoolExecutor(max_workers=Config.MAX_WORKERS) as executor:
            results = list(executor.map(index_batch, batches))
        
        success_count = sum(1 for r in results if r)
        print(f"‚úÖ Successfully indexed {success_count}/{len(batches)} batches", flush=True)
        
        return total


# ============================================================================
# SEARCH ENGINE
# ============================================================================

class SearchEngine:
    """Handles multi-level semantic search with re-ranking and query expansion"""
    
    def __init__(self):
        self.embeddings = OllamaEmbeddings(
            base_url=Config.OLLAMA_BASE_URL,
            model=Config.EMBEDDING_MODEL
        )
        self._vectordb = None
        self.query_expander = QueryExpander()
        self.llm = Ollama(
            base_url=Config.OLLAMA_BASE_URL,
            model=Config.OLLAMA_MODEL,
            temperature=0.1,
            num_predict=50,
        )
    
    @property
    def vectordb(self):
        """Lazy load vector database"""
        if self._vectordb is None and os.path.exists(Config.PERSIST_DIRECTORY):
            self._vectordb = Chroma(
                persist_directory=Config.PERSIST_DIRECTORY,
                embedding_function=self.embeddings
            )
        return self._vectordb
    
    def rerank_results(self, query: str, results: List[Tuple[Any, float]]) -> List[Tuple[Any, float]]:
        """Re-rank results using LLM to improve relevance ordering"""
        if len(results) <= Config.SEARCH_K_RERANK:
            return results
        
        try:
            # Build context for re-ranking - use more content for better judgment
            docs_text = "\n\n---\n\n".join([
                f"[{i}] FILE: {results[i][0].metadata.get('original_filename', 'Unknown')}\nCONTENT: {results[i][0].page_content[:500]}" 
                for i in range(min(15, len(results)))
            ])
            
            prompt = f"""You are a document relevance expert. Rank the documents by how relevant they are to the user's query.

USER QUERY: "{query}"

DOCUMENTS:
{docs_text}

TASK: Identify documents that are relevant to the query.
- INCLUDE: Documents that contain information about the topic in the query
- INCLUDE: Documents that mention related terms, policies, or concepts
- INCLUDE: Documents from files whose name suggests relevance (e.g., "leave.pdf" for leave queries)
- EXCLUDE ONLY: Documents that are completely unrelated (e.g., dress code docs for a leave query)

Be INCLUSIVE - if a document might be relevant, include it. Better to include slightly related docs than miss important ones.

Return the indices of relevant documents, ordered by relevance (most relevant first), comma-separated.
If truly NO documents are relevant at all, return: NONE

Your response (indices only, comma-separated):"""
            
            response = self.llm.invoke(prompt).strip().upper()
            
            # Check for NONE response
            if response == "NONE" or not response:
                print(f"‚ö†Ô∏è  Re-ranker found no relevant documents for query: {query}", flush=True)
                # Fallback: return top results by vector similarity instead of empty
                print(f"   Falling back to vector similarity results", flush=True)
                return results[:Config.SEARCH_K_RERANK]
            
            # Parse ranking
            ranking = [int(x.strip()) for x in response.split(',') if x.strip().isdigit()]
            
            # If no valid ranking, fallback to vector similarity
            if not ranking:
                print(f"‚ö†Ô∏è  Re-ranker returned no indices, using vector similarity", flush=True)
                return results[:Config.SEARCH_K_RERANK]
            
            # Reorder results based on ranking (only include relevant ones)
            reranked = []
            for idx in ranking:
                if 0 <= idx < len(results):
                    reranked.append(results[idx])
            
            # If reranking produced results, use them; otherwise fallback
            if reranked:
                print(f"‚úÖ Re-ranked to {len(reranked)} relevant documents", flush=True)
                return reranked[:Config.SEARCH_K_RERANK]
            else:
                print(f"‚ö†Ô∏è  Re-ranking produced no valid results, using vector similarity", flush=True)
                return results[:Config.SEARCH_K_RERANK]
        except Exception as e:
            print(f"‚ö†Ô∏è  Re-ranking failed: {e}, using vector similarity", flush=True)
            return results[:Config.SEARCH_K_RERANK]  # Fallback to vector similarity on error
    
    def search(self, query: str, k: int = None) -> List[SearchResult]:
        """Multi-level search with query expansion, re-ranking, and relevance filtering"""
        if not self.vectordb:
            print("‚ùå Vector database not found", flush=True)
            return []
        
        k = k or Config.SEARCH_K_RERANK
        
        # Step 1: Expand query for better coverage
        print(f"üîç Expanding query...", flush=True)
        expanded_queries = self.query_expander.expand_query(query)
        print(f"   Original: {query}", flush=True)
        print(f"   Expanded: {expanded_queries[1:]}", flush=True)
        
        # Step 2: Multi-query search
        all_results = {}
        for q in expanded_queries:
            try:
                results_with_scores = self.vectordb.similarity_search_with_score(
                    q, 
                    k=Config.SEARCH_K_INITIAL
                )
                
                for doc, score in results_with_scores:
                    doc_id = doc.metadata.get('doc_id', 'unknown')
                    chunk_idx = doc.metadata.get('chunk_index', 0)
                    key = f"{doc_id}_{chunk_idx}"
                    
                    # Keep best score for each document
                    if key not in all_results or score < all_results[key][1]:
                        all_results[key] = (doc, score)
            except Exception as e:
                print(f"‚ö†Ô∏è  Search failed for query '{q}': {e}", flush=True)
        
        # Step 3: Sort by score and re-rank
        sorted_results = sorted(all_results.values(), key=lambda x: x[1])
        print(f"üìä Found {len(sorted_results)} unique chunks across all queries", flush=True)
        
        # Log which documents were found
        found_docs = {}
        for doc, score in sorted_results:
            filename = doc.metadata.get('original_filename', 'Unknown')
            if filename not in found_docs:
                found_docs[filename] = 0
            found_docs[filename] += 1
        
        for filename, count in found_docs.items():
            print(f"   - {filename}: {count} chunks", flush=True)
        
        # Step 4: Re-rank for better relevance ordering (if we have enough results)
        if len(sorted_results) > Config.SEARCH_K_RERANK:
            print(f"üîÑ Re-ranking results...", flush=True)
            sorted_results = self.rerank_results(query, sorted_results)
        elif len(sorted_results) > 0:
            # Not enough results to re-rank, just take top K
            print(f"üìä Using top {min(len(sorted_results), Config.SEARCH_K_RERANK)} results by vector similarity", flush=True)
            sorted_results = sorted_results[:Config.SEARCH_K_RERANK]
        
        # Step 6: Format results
        search_results = []
        for i, (doc, score) in enumerate(sorted_results[:k]):
            # Normalize score (Chroma uses distance, lower is better)
            normalized_score = max(0, 1 - (score / 100))
            
            citation = Citation(
                document_name=doc.metadata.get('original_filename', 'Unknown'),
                page_number=doc.metadata.get('page', None),
                chunk_index=doc.metadata.get('chunk_index', 0),
                relevance_score=round(normalized_score, 3),
                content_preview=doc.page_content[:100] + "..."
            )
            
            search_results.append(SearchResult(
                content=doc.page_content,
                citation=citation,
                metadata=doc.metadata
            ))
        
        print(f"‚úÖ Returning {len(search_results)} relevant results", flush=True)
        return search_results


# ============================================================================
# RESPONSE GENERATOR
# ============================================================================

class ResponseGenerator:
    """Generates responses with citations using LLM"""
    
    def __init__(self):
        self.llm = Ollama(
            base_url=Config.OLLAMA_BASE_URL,
            model=Config.OLLAMA_MODEL,
            temperature=Config.TEMPERATURE,
            num_predict=Config.MAX_TOKENS,
            num_ctx=Config.CONTEXT_WINDOW
        )
    
    def build_context(self, results: List[SearchResult]) -> Tuple[str, List[Citation]]:
        """Build context string with citation markers"""
        if not results:
            return "", []
        
        context_parts = []
        citations = []
        
        for i, result in enumerate(results):
            citation_marker = f"[{i+1}]"
            context_parts.append(f"{citation_marker} {result.content}")
            citations.append(result.citation)
        
        return "\n\n---\n\n".join(context_parts), citations
    
    def build_prompt(self, query: str, context: str, conversation_history: List[Dict] = None) -> str:
        """Build the prompt for the LLM"""
        
        # Build conversation context
        conv_context = ""
        if conversation_history:
            recent = conversation_history[-4:]  # Last 4 messages
            conv_context = "\n\nRecent conversation:\n" + "\n".join([
                f"{m['role'].upper()}: {m['content'][:200]}" for m in recent
            ])
        
        # Check if context is meaningful
        has_context = context and len(context.strip()) > 50
        
        if has_context:
            prompt = f"""You are a helpful assistant. Your job is to answer questions based ONLY on the provided document context.

IMPORTANT: The context below contains the most relevant information. Use it to answer the question comprehensively and thoroughly.

CONTEXT FROM DOCUMENTS:
{context}
{conv_context}

QUESTION: {query}

INSTRUCTIONS:
- Answer the question using ALL relevant information in the context above
- Include citation numbers [1], [2], etc. when referencing information
- Provide a comprehensive answer covering all points and details mentioned in the context
- If the context contains a list or multiple items, include ALL of them in your answer
- Be direct, thorough, and complete
- Do not make up information not in the context

ANSWER:"""
        else:
            prompt = f"""You are a helpful assistant. A user asked a question but the available documents don't contain relevant information to answer it.

QUESTION: {query}

INSTRUCTIONS:
- Politely explain that the information is not available in the uploaded documents
- Suggest what type of documents might contain this information
- Do not make up information

ANSWER:"""
        
        return prompt
    
    def format_citations(self, citations: List[Citation]) -> str:
        """Format citations for display"""
        if not citations:
            return ""
        
        lines = ["\n\nüìö **Sources:**"]
        for i, c in enumerate(citations):
            page_info = f", Page {c.page_number}" if c.page_number else ""
            relevance = int(c.relevance_score*100) if c.relevance_score > 0 else 100
            lines.append(f"[{i+1}] **{c.document_name}**{page_info} (Relevance: {relevance}%)")
            lines.append(f"    üìÑ Preview: {c.content_preview}")
            lines.append(f"    üîó [View Document](#view-doc-{i+1})")
        
        return "\n".join(lines)
    
    def generate(self, query: str, results: List[SearchResult], 
                 conversation_history: List[Dict] = None, stream: bool = False):
        """Generate response with optional streaming"""
        
        context, citations = self.build_context(results)
        prompt = self.build_prompt(query, context, conversation_history)
        
        if stream:
            return self._stream_response(prompt, citations)
        else:
            return self._generate_response(prompt, citations)
    
    def _generate_response(self, prompt: str, citations: List[Citation]) -> Tuple[str, str]:
        """Generate non-streaming response"""
        answer = self.llm.invoke(prompt)
        citation_text = self.format_citations(citations)
        return answer + citation_text, "documents"
    
    def _stream_response(self, prompt: str, citations: List[Citation]) -> Generator:
        """Generate streaming response"""
        # Stream the main answer
        for chunk in self.llm.stream(prompt):
            yield (chunk, "documents", False)
        
        # Send citations at the end
        citation_text = self.format_citations(citations)
        if citation_text:
            yield (citation_text, "documents", False)
        
        yield ("", "documents", True)


# ============================================================================
# SINGLETON INSTANCES
# ============================================================================

_processor = None
_search_engine = None
_generator = None
_lock = threading.Lock()

def get_processor() -> DocumentProcessor:
    global _processor
    if _processor is None:
        with _lock:
            if _processor is None:
                _processor = DocumentProcessor()
    return _processor

def get_search_engine() -> SearchEngine:
    global _search_engine
    if _search_engine is None:
        with _lock:
            if _search_engine is None:
                _search_engine = SearchEngine()
    return _search_engine

def get_generator() -> ResponseGenerator:
    global _generator
    if _generator is None:
        with _lock:
            if _generator is None:
                _generator = ResponseGenerator()
    return _generator


# ============================================================================
# PUBLIC API FUNCTIONS
# ============================================================================

def process_document(file_path: str, file_type: str = None) -> dict:
    """
    Process a document and add it to the vector database.
    
    Args:
        file_path: Path to the document file
        file_type: File extension (pdf, docx, txt). Auto-detected if None.
    
    Returns:
        dict: {
            "success": bool,
            "message": str,
            "num_chunks": int,
            "processing_time": float
        }
    """
    start_time = time.time()
    
    try:
        # Auto-detect file type
        if not file_type:
            _, ext = os.path.splitext(file_path)
            file_type = ext.lower().replace('.', '')
        
        processor = get_processor()
        
        # Step 1: Load document
        print(f"üìÑ Loading {file_type.upper()} document...", flush=True)
        documents = processor.load_document(file_path, file_type)
        
        if not documents:
            return {
                "success": False,
                "message": "No content extracted from document",
                "num_chunks": 0,
                "processing_time": time.time() - start_time
            }
        
        print(f"‚úì Loaded {len(documents)} pages in {time.time() - start_time:.1f}s", flush=True)
        
        # Step 2: Chunk documents
        print(f"üìù Chunking document...", flush=True)
        chunks = processor.chunk_documents(documents, file_path, file_type)
        print(f"‚úì Created {len(chunks)} chunks", flush=True)
        
        # Step 3: Index chunks
        print(f"üîç Indexing chunks...", flush=True)
        num_indexed = processor.index_chunks(chunks)
        
        processing_time = time.time() - start_time
        print(f"‚úÖ Document processed in {processing_time:.1f}s", flush=True)
        
        return {
            "success": True,
            "message": f"Processed {num_indexed} chunks in {processing_time:.1f}s",
            "num_chunks": num_indexed,
            "processing_time": processing_time
        }
    
    except Exception as e:
        return {
            "success": False,
            "message": f"Error: {str(e)}",
            "num_chunks": 0,
            "processing_time": time.time() - start_time
        }


def is_greeting_or_smalltalk(query: str) -> tuple:
    """
    Detect if the query is a greeting or small talk that doesn't need document search.
    
    Returns:
        tuple: (is_smalltalk: bool, response: str or None)
    """
    query_lower = query.lower().strip()
    
    # Common greetings
    greetings = [
        'hi', 'hello', 'hey', 'hii', 'hiii', 'hiiii',
        'good morning', 'good afternoon', 'good evening', 'good night',
        'howdy', 'greetings', 'sup', 'whats up', "what's up",
        'yo', 'hola', 'bonjour', 'namaste'
    ]
    
    # Small talk patterns
    smalltalk_patterns = [
        r'^hi+$', r'^hey+$', r'^hello+$',
        r'^how are you', r'^how r u', r'^how do you do',
        r'^what\'?s up', r'^whats up', r'^wassup',
        r'^good (morning|afternoon|evening|night)',
        r'^thanks?$', r'^thank you', r'^thx$',
        r'^bye', r'^goodbye', r'^see you', r'^later',
        r'^ok$', r'^okay$', r'^cool$', r'^nice$',
        r'^yes$', r'^no$', r'^yep$', r'^nope$',
    ]
    
    # Check exact greetings
    if query_lower in greetings:
        return (True, "Hello! üëã I'm your AI Assistant. Upload documents and ask me anything about them. I'll provide answers with precise citations.")
    
    # Check patterns
    for pattern in smalltalk_patterns:
        if re.match(pattern, query_lower):
            if 'how are you' in query_lower or 'how r u' in query_lower:
                return (True, "I'm doing great, thanks for asking! üòä How can I help you with your documents today?")
            elif 'thank' in query_lower or 'thx' in query_lower:
                return (True, "You're welcome! üòä Let me know if you have any other questions about your documents.")
            elif 'bye' in query_lower or 'goodbye' in query_lower or 'see you' in query_lower:
                return (True, "Goodbye! üëã Feel free to come back anytime you need help with your documents.")
            else:
                return (True, "Hello! üëã I'm your AI Assistant. Upload documents and ask me anything about them. I'll provide answers with precise citations.")
    
    return (False, None)


def get_answer_with_fallback(query: str, conversation_history: list = None, stream: bool = False):
    """
    Answer questions using uploaded documents.
    
    Args:
        query: The user's question
        conversation_history: List of previous messages [{"role": "user/assistant", "content": "..."}]
        stream: If True, returns a generator for streaming responses
    
    Returns:
        If stream=False: tuple (answer: str, source: str)
        If stream=True: generator yielding (chunk: str, source: str, done: bool)
    """
    try:
        # Check for greetings and small talk first
        is_smalltalk, smalltalk_response = is_greeting_or_smalltalk(query)
        if is_smalltalk:
            if stream:
                yield (smalltalk_response, "greeting", True)
                return
            return (smalltalk_response, "greeting")
        
        # Check if documents exist
        if not os.path.exists(Config.PERSIST_DIRECTORY):
            no_docs_msg = "‚ùå No documents uploaded yet. Please upload documents first."
            if stream:
                yield (no_docs_msg, "no_documents", True)
                return
            return (no_docs_msg, "no_documents")
        
        search_engine = get_search_engine()
        generator = get_generator()
        
        # Search for relevant documents
        print(f"üîç Searching for: '{query}'", flush=True)
        results = search_engine.search(query)
        
        if not results:
            no_results_msg = "‚ùå I couldn't find relevant information in the uploaded documents. Try rephrasing your question or upload more documents."
            if stream:
                yield (no_results_msg, "no_documents", True)
                return
            return (no_results_msg, "no_documents")
        
        print(f"‚úì Found {len(results)} relevant chunks", flush=True)
        for i, r in enumerate(results):
            print(f"  [{i+1}] {r.citation.document_name} (score: {r.citation.relevance_score})", flush=True)
        
        # Generate response
        if stream:
            yield from generator.generate(query, results, conversation_history, stream=True)
        else:
            return generator.generate(query, results, conversation_history, stream=False)
    
    except Exception as e:
        error_msg = f"‚ùå Error: {str(e)}"
        if stream:
            yield (error_msg, "error", True)
        else:
            return (error_msg, "error")


# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def clear_database():
    """Clear all documents from the vector database"""
    import shutil
    if os.path.exists(Config.PERSIST_DIRECTORY):
        shutil.rmtree(Config.PERSIST_DIRECTORY)
        print("‚úì Database cleared", flush=True)
        return True
    return False


def get_document_count() -> int:
    """Get the number of documents in the database"""
    search_engine = get_search_engine()
    if search_engine.vectordb:
        try:
            return search_engine.vectordb._collection.count()
        except Exception as e:
            print(f"Error getting document count: {e}")
    return 0


def delete_document(file_path: str) -> bool:
    """Delete a specific document from the database"""
    search_engine = get_search_engine()
    if search_engine.vectordb:
        try:
            search_engine.vectordb.delete(where={"source_file": file_path})
            return True
        except Exception as e:
            print(f"Error deleting document: {e}")
    return False


def test_search(query: str) -> None:
    """Test search functionality"""
    print(f"\nüß™ Testing search for: '{query}'", flush=True)
    search_engine = get_search_engine()
    results = search_engine.search(query)
    print(f"üìä Found {len(results)} results", flush=True)
    for i, result in enumerate(results):
        print(f"  [{i+1}] {result.citation.document_name}: {result.content[:100]}...", flush=True)
